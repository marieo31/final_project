{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle different types of transformation\n",
    "# different nb of pixels\n",
    "# different nb of level of gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MariO\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# from sklearn import tree\n",
    "import os\n",
    "import random\n",
    "\n",
    "mpl.rc('image', cmap='gray_r')\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rot90ccw(mat):\n",
    "    \"\"\" Rotate an n by n matrix 90 deg counter clock wise\"\"\"\n",
    "    \n",
    "    # rotation matrix\n",
    "    rot = np.array([[0, 1],\n",
    "               [-1,0]])\n",
    "    # matrix coordinates\n",
    "    coord = [i for i in np.ndindex(mat.shape)]\n",
    "    nbp = mat.shape[0]    \n",
    "    # initialisations\n",
    "    coord_rot = []\n",
    "    mat_rot = np.zeros((mat.shape))    \n",
    "    # Apply the transformation on each pixel's coordinates\n",
    "    for ii in range(0,len(coord)):\n",
    "        # apply the rotation to the coordinate vector\n",
    "        coord_rot.append(coord[ii]@rot)\n",
    "        # reverse the row axis\n",
    "        coord_rot[ii][0] += nbp-1\n",
    "        # transform back into a tuple\n",
    "        coord_rot[ii] = tuple(coord_rot[ii])   \n",
    "        # fill up the rotated matrix\n",
    "        mat_rot[coord_rot[ii]] = mat[coord[ii]]        \n",
    "    \n",
    "    return mat_rot\n",
    "\n",
    "def rot90cw(mat):\n",
    "    \"\"\" Rotate an n by n matrix 90 deg clock wise\"\"\"\n",
    "    \n",
    "    # rotation matrix\n",
    "    rot = np.array([[0, -1],\n",
    "               [1,0]])    \n",
    "    # matrix coordinates\n",
    "    coord = [i for i in np.ndindex(mat.shape)]\n",
    "    nbp = mat.shape[0]    \n",
    "    # initialisations\n",
    "    coord_rot = []\n",
    "    mat_rot = np.zeros((mat.shape))    \n",
    "    # Apply the transformation on each pixel's coordinates\n",
    "    for ii in range(0,len(coord)):\n",
    "        # apply the rotation to the coordinate vector\n",
    "        coord_rot.append(coord[ii]@rot)\n",
    "        # reverse the row axis\n",
    "        coord_rot[ii][1] += nbp-1\n",
    "        # transform back into a tuple\n",
    "        coord_rot[ii] = tuple(coord_rot[ii])   \n",
    "        # fill up the rotated matrix\n",
    "        mat_rot[coord_rot[ii]] = mat[coord[ii]]        \n",
    "    \n",
    "    return mat_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation matrices\n",
    "def create_PPstar_translation(nbpix):\n",
    "    # nb of pixels translated\n",
    "    nbt = int(np.round(0.25*nbpix))\n",
    "\n",
    "    # bottom left square block\n",
    "    bl = np.eye(nbpix-nbt,nbpix-nbt)\n",
    "    # upper right square block\n",
    "    ur = np.eye(nbt,nbt)\n",
    "    # upper left rect block\n",
    "    ul = np.zeros((nbt,nbpix-nbt))\n",
    "    # bottom right rect block\n",
    "    br = np.zeros((nbpix-nbt,nbt))\n",
    "\n",
    "    # concatenate the blocks to build the transformation matrix\n",
    "    P = np.concatenate((np.concatenate((ul,ur), axis=1), np.concatenate((bl,br), axis=1)), axis=0)\n",
    "    Pstar = np.linalg.inv(P)        \n",
    "    return (P,Pstar)\n",
    "\n",
    "def translate2left(mat):\n",
    "    P,Pstar = create_PPstar_translation(mat.shape[0])\n",
    "    return mat@P\n",
    "def translate2right(mat):\n",
    "    P,Pstar = create_PPstar_translation(mat.shape[0])\n",
    "    return mat@Pstar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "def remove_ticks():\n",
    "    plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "    plt.tick_params(\n",
    "    axis='y',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    left=False,\n",
    "    labelleft=False,\n",
    "    labelbottom=False) # labels along the bottom edge are off    \n",
    "    return\n",
    "\n",
    "def vis_matrices(Mr, Mm, Mst, MstM):\n",
    "#     print(Mr.shape[0])\n",
    "    if Mr.shape[0]==1:\n",
    "        Mr = Mr.reshape(int(np.sqrt(Mr.shape[1])), int(np.sqrt(Mr.shape[1])))\n",
    "        Mm = Mm.reshape(int(np.sqrt(Mm.shape[1])), int(np.sqrt(Mm.shape[1])))\n",
    "        Mst = Mst.reshape(int(np.sqrt(Mst.shape[1])), int(np.sqrt(Mst.shape[1])))\n",
    "        MstM = MstM.reshape(int(np.sqrt(Mst.shape[1])), int(np.sqrt(Mst.shape[1])))\n",
    "\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(nrows=2,ncols=2) #, squeeze=True, sharey=True)\n",
    "    fig.set_size_inches(8,8)\n",
    "\n",
    "    plt.sca(ax1[0])\n",
    "    plt.imshow(Mr)\n",
    "    plt.title(\"Real image\")\n",
    "    remove_ticks()\n",
    "\n",
    "    plt.sca(ax2[0])\n",
    "    plt.imshow(Mm)\n",
    "    plt.title(\"Distorted image\")\n",
    "    remove_ticks()\n",
    "\n",
    "    plt.sca(ax1[1])\n",
    "    plt.imshow(Mst)\n",
    "    plt.title(\"Corrected input\")\n",
    "    remove_ticks()\n",
    "\n",
    "    plt.sca(ax2[1])\n",
    "    plt.imshow(MstM)\n",
    "    plt.title(\"Corrected visualization\")\n",
    "    remove_ticks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(nbpix,transform_type):\n",
    "    \"\"\"\n",
    "    Build the set of one pixel matrices and apply the transformations\n",
    "    \"\"\"\n",
    "    # List of indexes where to put a black pixel\n",
    "    idx = range(0,nbpix**2,1)\n",
    "\n",
    "    # Initialize empty arrays \n",
    "    Mreal_tab = np.zeros((len(idx),nbpix*nbpix))\n",
    "    Mmang_tab = np.zeros((len(idx),nbpix*nbpix))\n",
    "    Mstar_tab = np.zeros((len(idx),nbpix*nbpix))\n",
    "    for ii in idx:\n",
    "        # Fill the indexed pixel with a one\n",
    "        Mreal_tab[ii,ii] = 1\n",
    "        # Use the transformation matrices to generate Mmang and Mstar\n",
    "#         Mmang_tab[ii,:] = (Mreal_tab[ii,:].reshape(nbpix,nbpix)@P).reshape(1,nbpix**2)\n",
    "#         Mstar_tab[ii,:] = (Mreal_tab[ii,:].reshape(nbpix,nbpix)@Pstar).reshape(1,nbpix**2)\n",
    "            \n",
    "        if transform_type is \"translation\":    \n",
    "            Mmang_tab[ii,:] = translate2left(Mreal_tab[ii,:].reshape(nbpix,nbpix)).reshape(1,nbpix**2)\n",
    "            Mstar_tab[ii,:] = translate2right(Mreal_tab[ii,:].reshape(nbpix,nbpix)).reshape(1,nbpix**2)\n",
    "        elif transform_type is \"rotation\":\n",
    "            Mmang_tab[ii,:] = rot90ccw(Mreal_tab[ii,:].reshape(nbpix,nbpix)).reshape(1,nbpix**2)\n",
    "            Mstar_tab[ii,:] = rot90cw(Mreal_tab[ii,:].reshape(nbpix,nbpix)).reshape(1,nbpix**2)\n",
    "            \n",
    "    return (Mreal_tab, Mmang_tab, Mstar_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add one on the designated index\n",
    "def add_one(arr,index):\n",
    "    arr[0,index] = 1\n",
    "    return arr\n",
    "\n",
    "# Function to split the matrix into matrices of one pixel\n",
    "def split_matrix(mat):\n",
    "    \"\"\" Split a matrix into matrices of one pixel\"\"\"\n",
    "    # find the indexes of the ones values in the matrix\n",
    "#     idx_arr = np.where(mat.reshape(1,nbpix**2)[0]==1)[0]\n",
    "    idx_arr = np.where(mat.reshape(1,nbpix**2)[0]>0)[0]\n",
    "    # transform from an array to a list\n",
    "    idx_lst = [idx_arr.item(ii) for ii in range(0,len(idx_arr))] \n",
    "    temp = np.zeros((1,nbpix**2))\n",
    "    mat_split = np.zeros((len(idx_lst), nbpix**2))\n",
    "#     Mtest = np.zeros((nbpix,nbpix))\n",
    "    for ii in range(0,len(idx_lst)):\n",
    "        mat_split[ii,idx_lst[ii]] = 1    \n",
    "#         Mtest = Mtest + Mreal_split[ii,:].reshape(nbpix,nbpix)\n",
    "    return mat_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction functions\n",
    "def perform_prediction(mat, model):\n",
    "    \"\"\" Predict the output with a trained model\"\"\"\n",
    "    # split the input matrix into matrices of one pixel\n",
    "    mat_split = split_matrix(mat)\n",
    "    nb_dark_pxl = mat_split.shape[0]\n",
    "    \n",
    "    # initializations\n",
    "    res = np.zeros((nb_dark_pxl, nbpix**2))\n",
    "    Mres = np.zeros((nbpix,nbpix))\n",
    "    # Loop on all the one pixels array\n",
    "    for ii in range(0,nb_dark_pxl):\n",
    "        # Apply the models to the one pixel matrices\n",
    "        res[ii,:] = model.predict(np.expand_dims(mat_split[ii], axis=0))\n",
    "        # Sum up the results to construct the matrices\n",
    "        Mres = Mres + res[ii,:].reshape(nbpix,nbpix)\n",
    "    return Mres\n",
    "\n",
    "# def predict_corrector(mat):\n",
    "#     # split the input matrix into matrices of one pixel\n",
    "#     mat_split = split_matrix(mat)\n",
    "#     nb_dark_pxl = mat_split.shape[0]\n",
    "    \n",
    "#     # initializations\n",
    "#     res = np.zeros((nb_dark_pxl, nbpix**2))\n",
    "#     Mres = np.zeros((nbpix,nbpix))\n",
    "#     # Loop on all the one pixels array\n",
    "#     for ii in range(0,nb_dark_pxl):\n",
    "#         # Apply the models to the one pixel matrices\n",
    "#         res[ii,:] = model_corrector.predict(np.expand_dims(mat_split[ii], axis=0))\n",
    "#         # Sum up the results to construct the matrices\n",
    "#         Mres = Mres + res[ii,:].reshape(nbpix,nbpix)\n",
    "#     return Mres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_door_img(nbpix):\n",
    "    \"\"\" Create an nbpix by nbpix image of a door\"\"\"\n",
    "    nbborder = int(np.round(nbpix*0.3))\n",
    "    u = np.ones((nbborder,nbpix))\n",
    "    l = np.ones((nbpix-nbborder,nbborder))\n",
    "    r = l\n",
    "    door = np.zeros((nbpix-nbborder, nbpix-2*nbborder))\n",
    "\n",
    "    return np.concatenate((u,np.concatenate((l,door,r), axis=1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def grayscale_cmap(cmap):\n",
    "#     \"\"\"Return a grayscale version of the given colormap\"\"\"\n",
    "#     cmap = plt.cm.get_cmap(cmap)\n",
    "#     colors = cmap(np.arange(cmap.N))\n",
    "    \n",
    "#     # convert RGBA to perceived grayscale luminance\n",
    "#     # cf. http://alienryderflex.com/hsp.html\n",
    "#     RGB_weight = [0.299, 0.587, 0.114]\n",
    "#     luminance = np.sqrt(np.dot(colors[:, :3] ** 2, RGB_weight))\n",
    "#     colors[:, :3] = luminance[:, np.newaxis]\n",
    "        \n",
    "#     return LinearSegmentedColormap.from_list(cmap.name + \"_gray\", colors, cmap.N)\n",
    "\n",
    "# mpl.rc('image', cmap=grayscale_cmap(\"viridis_r\"))\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "nbpix = 4 # nb of pixel to consider (nbpix x nbpix)\n",
    "transform_type = \"rotation\" # type of transformation : \"translation\" or \"rotation\"\n",
    "training_type = \"full\" # type of training: \"partial\" of \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformation matrices\n",
    "\n",
    "# import random\n",
    "# (P,Pstar) = create_PPstar(nbpix)\n",
    "\n",
    "# Create the training data\n",
    "(Mreal_tab, Mmang_tab, Mstar_tab) = create_training_data(nbpix,transform_type)\n",
    "\n",
    "\n",
    "# the training input are the matrices of the real images with one pixel\n",
    "\n",
    "if training_type is \"partial\": # we remove randomly 20% of the training data\n",
    "    # rebuild the list of indexes and shuffle it\n",
    "    rdm_idx = list(range(0,nbpix**2,1))\n",
    "    random.shuffle(rdm_idx)\n",
    "    \n",
    "    # remove 80% of the indexes of this list\n",
    "    rdm_idx[0:int(np.round(nbpix**2*0.8))] = []\n",
    "    \n",
    "    # delete the corresponding rows in the training matrices\n",
    "    Mreal_tab = np.delete(Mreal_tab, rdm_idx, axis=0)\n",
    "    Mmang_tab = np.delete(Mmang_tab, rdm_idx, axis=0)\n",
    "    Mstar_tab = np.delete(Mstar_tab, rdm_idx, axis=0)\n",
    "    \n",
    "    print(f\"Training performed on {Mreal_tab.shape[0]}/{Mreal_tab.shape[1]} pixels\")\n",
    "\n",
    "idx = range(0,Mreal_tab.shape[0],1)        \n",
    "X_train = Mreal_tab   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 2.7749 - acc: 0.0625\n",
      "Epoch 2/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.7538 - acc: 0.1250\n",
      "Epoch 3/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.7336 - acc: 0.1250\n",
      "Epoch 4/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.7139 - acc: 0.1875\n",
      "Epoch 5/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6945 - acc: 0.1875\n",
      "Epoch 6/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6755 - acc: 0.1875\n",
      "Epoch 7/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.6566 - acc: 0.1875\n",
      "Epoch 8/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.6379 - acc: 0.1875\n",
      "Epoch 9/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.6190 - acc: 0.3125\n",
      "Epoch 10/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6000 - acc: 0.5000\n",
      "Epoch 11/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 2.5807 - acc: 0.5625\n",
      "Epoch 12/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.5613 - acc: 0.6875\n",
      "Epoch 13/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.5415 - acc: 0.8125\n",
      "Epoch 14/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.5215 - acc: 0.8125\n",
      "Epoch 15/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.5009 - acc: 0.8750\n",
      "Epoch 16/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.4800 - acc: 0.9375\n",
      "Epoch 17/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.4589 - acc: 0.9375\n",
      "Epoch 18/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.4374 - acc: 1.0000\n",
      "Epoch 19/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.4155 - acc: 1.0000\n",
      "Epoch 20/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.3929 - acc: 1.0000\n",
      "Epoch 21/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.3698 - acc: 1.0000\n",
      "Epoch 22/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.3462 - acc: 1.0000\n",
      "Epoch 23/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.3220 - acc: 1.0000\n",
      "Epoch 24/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.2973 - acc: 1.0000\n",
      "Epoch 25/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.2721 - acc: 1.0000\n",
      "Epoch 26/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.2458 - acc: 1.0000\n",
      "Epoch 27/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.2192 - acc: 1.0000\n",
      "Epoch 28/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.1922 - acc: 1.0000\n",
      "Epoch 29/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.1645 - acc: 1.0000\n",
      "Epoch 30/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 2.1360 - acc: 1.0000\n",
      "Epoch 31/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.1065 - acc: 1.0000\n",
      "Epoch 32/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.0761 - acc: 1.0000\n",
      "Epoch 33/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.0448 - acc: 1.0000\n",
      "Epoch 34/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.0128 - acc: 1.0000\n",
      "Epoch 35/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.9801 - acc: 1.0000\n",
      "Epoch 36/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.9467 - acc: 1.0000\n",
      "Epoch 37/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.9127 - acc: 1.0000\n",
      "Epoch 38/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.8780 - acc: 1.0000\n",
      "Epoch 39/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.8425 - acc: 1.0000\n",
      "Epoch 40/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.8063 - acc: 1.0000\n",
      "Epoch 41/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 1.7692 - acc: 1.0000\n",
      "Epoch 42/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.7314 - acc: 1.0000\n",
      "Epoch 43/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 1.6929 - acc: 1.0000\n",
      "Epoch 44/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.6536 - acc: 1.0000\n",
      "Epoch 45/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.6136 - acc: 1.0000\n",
      "Epoch 46/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.5731 - acc: 1.0000\n",
      "Epoch 47/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.5316 - acc: 1.0000\n",
      "Epoch 48/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.4899 - acc: 1.0000\n",
      "Epoch 49/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.4477 - acc: 1.0000\n",
      "Epoch 50/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.4051 - acc: 1.0000\n",
      "Epoch 51/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.3618 - acc: 1.0000\n",
      "Epoch 52/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.3185 - acc: 1.0000\n",
      "Epoch 53/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.2745 - acc: 1.0000\n",
      "Epoch 54/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.2307 - acc: 1.0000\n",
      "Epoch 55/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.1867 - acc: 1.0000\n",
      "Epoch 56/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.1429 - acc: 1.0000\n",
      "Epoch 57/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 1.0992 - acc: 1.0000\n",
      "Epoch 58/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.0556 - acc: 1.0000\n",
      "Epoch 59/240\n",
      "16/16 [==============================] - 0s 438us/step - loss: 1.0121 - acc: 1.0000\n",
      "Epoch 60/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.9690 - acc: 1.0000\n",
      "Epoch 61/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 0.9262 - acc: 1.0000\n",
      "Epoch 62/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.8839 - acc: 1.0000\n",
      "Epoch 63/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.8423 - acc: 1.0000\n",
      "Epoch 64/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.8014 - acc: 1.0000\n",
      "Epoch 65/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.7614 - acc: 1.0000\n",
      "Epoch 66/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.7222 - acc: 1.0000\n",
      "Epoch 67/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6841 - acc: 1.0000\n",
      "Epoch 68/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6468 - acc: 1.0000\n",
      "Epoch 69/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6109 - acc: 1.0000\n",
      "Epoch 70/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.5761 - acc: 1.0000\n",
      "Epoch 71/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.5426 - acc: 1.0000\n",
      "Epoch 72/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.5106 - acc: 1.0000\n",
      "Epoch 73/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.4799 - acc: 1.0000\n",
      "Epoch 74/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.4506 - acc: 1.0000\n",
      "Epoch 75/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.4227 - acc: 1.0000\n",
      "Epoch 76/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3962 - acc: 1.0000\n",
      "Epoch 77/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.3712 - acc: 1.0000\n",
      "Epoch 78/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.3477 - acc: 1.0000\n",
      "Epoch 79/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3253 - acc: 1.0000\n",
      "Epoch 80/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.3045 - acc: 1.0000\n",
      "Epoch 81/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.2850 - acc: 1.0000\n",
      "Epoch 82/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.2668 - acc: 1.0000\n",
      "Epoch 83/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.2497 - acc: 1.0000\n",
      "Epoch 84/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.2338 - acc: 1.0000\n",
      "Epoch 85/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.2190 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.2053 - acc: 1.0000\n",
      "Epoch 87/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1925 - acc: 1.0000\n",
      "Epoch 88/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1807 - acc: 1.0000\n",
      "Epoch 89/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1698 - acc: 1.0000\n",
      "Epoch 90/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1597 - acc: 1.0000\n",
      "Epoch 91/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1504 - acc: 1.0000\n",
      "Epoch 92/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1417 - acc: 1.0000\n",
      "Epoch 93/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1337 - acc: 1.0000\n",
      "Epoch 94/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1263 - acc: 1.0000\n",
      "Epoch 95/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1194 - acc: 1.0000\n",
      "Epoch 96/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1130 - acc: 1.0000\n",
      "Epoch 97/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1071 - acc: 1.0000\n",
      "Epoch 98/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1017 - acc: 1.0000\n",
      "Epoch 99/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0966 - acc: 1.0000\n",
      "Epoch 100/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0919 - acc: 1.0000\n",
      "Epoch 101/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0876 - acc: 1.0000\n",
      "Epoch 102/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0835 - acc: 1.0000\n",
      "Epoch 103/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0797 - acc: 1.0000\n",
      "Epoch 104/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0762 - acc: 1.0000\n",
      "Epoch 105/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0729 - acc: 1.0000\n",
      "Epoch 106/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0699 - acc: 1.0000\n",
      "Epoch 107/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0670 - acc: 1.0000\n",
      "Epoch 108/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0644 - acc: 1.0000\n",
      "Epoch 109/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0619 - acc: 1.0000\n",
      "Epoch 110/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0595 - acc: 1.0000\n",
      "Epoch 111/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0573 - acc: 1.0000\n",
      "Epoch 112/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0553 - acc: 1.0000\n",
      "Epoch 113/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0533 - acc: 1.0000\n",
      "Epoch 114/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0515 - acc: 1.0000\n",
      "Epoch 115/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0498 - acc: 1.0000\n",
      "Epoch 116/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 117/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0466 - acc: 1.0000\n",
      "Epoch 118/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0452 - acc: 1.0000\n",
      "Epoch 119/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0438 - acc: 1.0000\n",
      "Epoch 120/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0425 - acc: 1.0000\n",
      "Epoch 121/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 122/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0401 - acc: 1.0000\n",
      "Epoch 123/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0390 - acc: 1.0000\n",
      "Epoch 124/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0379 - acc: 1.0000\n",
      "Epoch 125/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0369 - acc: 1.0000\n",
      "Epoch 126/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0360 - acc: 1.0000\n",
      "Epoch 127/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0351 - acc: 1.0000\n",
      "Epoch 128/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 129/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 130/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 0.0325 - acc: 1.0000\n",
      "Epoch 131/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 132/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0310 - acc: 1.0000\n",
      "Epoch 133/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0303 - acc: 1.0000\n",
      "Epoch 134/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 135/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 136/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0284 - acc: 1.0000\n",
      "Epoch 137/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 138/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0272 - acc: 1.0000\n",
      "Epoch 139/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0266 - acc: 1.0000\n",
      "Epoch 140/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 141/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0256 - acc: 1.0000\n",
      "Epoch 142/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0251 - acc: 1.0000\n",
      "Epoch 143/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 144/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0241 - acc: 1.0000\n",
      "Epoch 145/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0237 - acc: 1.0000\n",
      "Epoch 146/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0233 - acc: 1.0000\n",
      "Epoch 147/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 148/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0224 - acc: 1.0000\n",
      "Epoch 149/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 150/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 151/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 152/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0209 - acc: 1.0000\n",
      "Epoch 153/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 154/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 155/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 156/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 157/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 158/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0189 - acc: 1.0000\n",
      "Epoch 159/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0186 - acc: 1.0000\n",
      "Epoch 160/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 161/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 162/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 163/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.0175 - acc: 1.0000\n",
      "Epoch 164/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 165/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0170 - acc: 1.0000\n",
      "Epoch 166/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0167 - acc: 1.0000\n",
      "Epoch 167/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 168/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 169/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0160 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 171/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 172/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 173/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 174/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 175/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 176/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 177/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 178/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 179/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 180/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 181/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 182/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 183/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 184/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 185/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 186/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 187/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 188/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 189/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 190/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 191/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 192/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 193/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 194/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 195/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 196/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 197/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 198/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 199/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0110 - acc: 1.0000\n",
      "Epoch 200/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 201/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 202/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 203/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 204/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 205/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 206/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 207/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 208/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 209/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 210/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 211/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 212/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 213/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 214/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 215/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0092 - acc: 1.0000\n",
      "Epoch 216/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 217/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 218/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 219/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 220/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 221/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 222/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 223/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 224/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 225/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 226/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 227/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 228/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 229/240\n",
      "16/16 [==============================] - 0s 438us/step - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 230/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 231/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 232/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 233/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 234/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 235/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0076 - acc: 1.0000\n",
      "Epoch 236/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 237/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 238/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 239/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 240/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 1/240\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 2.7959 - acc: 0.0000e+00\n",
      "Epoch 2/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.7810 - acc: 0.0000e+00\n",
      "Epoch 3/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.7640 - acc: 0.0000e+00\n",
      "Epoch 4/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.7445 - acc: 0.0000e+00\n",
      "Epoch 5/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.7255 - acc: 0.2500\n",
      "Epoch 6/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.7071 - acc: 0.2500\n",
      "Epoch 7/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6888 - acc: 0.3125\n",
      "Epoch 8/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6706 - acc: 0.4375\n",
      "Epoch 9/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6521 - acc: 0.5000\n",
      "Epoch 10/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6338 - acc: 0.6250\n",
      "Epoch 11/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.6154 - acc: 0.6875\n",
      "Epoch 12/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.5970 - acc: 0.6875\n",
      "Epoch 13/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.5782 - acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 2.5593 - acc: 0.7500\n",
      "Epoch 15/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.5402 - acc: 0.8125\n",
      "Epoch 16/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.5207 - acc: 0.9375\n",
      "Epoch 17/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 2.5009 - acc: 0.9375\n",
      "Epoch 18/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.4809 - acc: 0.9375\n",
      "Epoch 19/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.4605 - acc: 0.9375\n",
      "Epoch 20/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.4395 - acc: 0.9375\n",
      "Epoch 21/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.4179 - acc: 1.0000\n",
      "Epoch 22/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.3956 - acc: 1.0000\n",
      "Epoch 23/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.3727 - acc: 1.0000\n",
      "Epoch 24/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.3492 - acc: 1.0000\n",
      "Epoch 25/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.3249 - acc: 1.0000\n",
      "Epoch 26/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 2.3002 - acc: 1.0000\n",
      "Epoch 27/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.2748 - acc: 1.0000\n",
      "Epoch 28/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.2487 - acc: 1.0000\n",
      "Epoch 29/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.2221 - acc: 1.0000\n",
      "Epoch 30/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.1947 - acc: 1.0000\n",
      "Epoch 31/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.1665 - acc: 1.0000\n",
      "Epoch 32/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.1375 - acc: 1.0000\n",
      "Epoch 33/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 2.1077 - acc: 1.0000\n",
      "Epoch 34/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 2.0772 - acc: 1.0000\n",
      "Epoch 35/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 2.0458 - acc: 1.0000\n",
      "Epoch 36/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 2.0132 - acc: 1.0000\n",
      "Epoch 37/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.9802 - acc: 1.0000\n",
      "Epoch 38/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.9465 - acc: 1.0000\n",
      "Epoch 39/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.9121 - acc: 1.0000\n",
      "Epoch 40/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.8768 - acc: 1.0000\n",
      "Epoch 41/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.8407 - acc: 1.0000\n",
      "Epoch 42/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.8040 - acc: 1.0000\n",
      "Epoch 43/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.7665 - acc: 1.0000\n",
      "Epoch 44/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.7283 - acc: 1.0000\n",
      "Epoch 45/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.6893 - acc: 1.0000\n",
      "Epoch 46/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.6499 - acc: 1.0000\n",
      "Epoch 47/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.6098 - acc: 1.0000\n",
      "Epoch 48/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.5692 - acc: 1.0000\n",
      "Epoch 49/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.5280 - acc: 1.0000\n",
      "Epoch 50/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.4863 - acc: 1.0000\n",
      "Epoch 51/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.4443 - acc: 1.0000\n",
      "Epoch 52/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.4020 - acc: 1.0000\n",
      "Epoch 53/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.3593 - acc: 1.0000\n",
      "Epoch 54/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.3164 - acc: 1.0000\n",
      "Epoch 55/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.2733 - acc: 1.0000\n",
      "Epoch 56/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.2301 - acc: 1.0000\n",
      "Epoch 57/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.1869 - acc: 1.0000\n",
      "Epoch 58/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.1436 - acc: 1.0000\n",
      "Epoch 59/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.1005 - acc: 1.0000\n",
      "Epoch 60/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 1.0576 - acc: 1.0000\n",
      "Epoch 61/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 1.0150 - acc: 1.0000\n",
      "Epoch 62/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.9728 - acc: 1.0000\n",
      "Epoch 63/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.9309 - acc: 1.0000\n",
      "Epoch 64/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.8897 - acc: 1.0000\n",
      "Epoch 65/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.8491 - acc: 1.0000\n",
      "Epoch 66/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.8091 - acc: 1.0000\n",
      "Epoch 67/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.7700 - acc: 1.0000\n",
      "Epoch 68/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.7319 - acc: 1.0000\n",
      "Epoch 69/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.6946 - acc: 1.0000\n",
      "Epoch 70/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.6583 - acc: 1.0000\n",
      "Epoch 71/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.6233 - acc: 1.0000\n",
      "Epoch 72/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.5893 - acc: 1.0000\n",
      "Epoch 73/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.5565 - acc: 1.0000\n",
      "Epoch 74/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.5249 - acc: 1.0000\n",
      "Epoch 75/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.4946 - acc: 1.0000\n",
      "Epoch 76/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.4656 - acc: 1.0000\n",
      "Epoch 77/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.4380 - acc: 1.0000\n",
      "Epoch 78/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.4117 - acc: 1.0000\n",
      "Epoch 79/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3867 - acc: 1.0000\n",
      "Epoch 80/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3631 - acc: 1.0000\n",
      "Epoch 81/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.3408 - acc: 1.0000\n",
      "Epoch 82/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.3197 - acc: 1.0000\n",
      "Epoch 83/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.2998 - acc: 1.0000\n",
      "Epoch 84/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.2812 - acc: 1.0000\n",
      "Epoch 85/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.2638 - acc: 1.0000\n",
      "Epoch 86/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.2475 - acc: 1.0000\n",
      "Epoch 87/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.2323 - acc: 1.0000\n",
      "Epoch 88/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.2182 - acc: 1.0000\n",
      "Epoch 89/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.2050 - acc: 1.0000\n",
      "Epoch 90/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1927 - acc: 1.0000\n",
      "Epoch 91/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1813 - acc: 1.0000\n",
      "Epoch 92/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1707 - acc: 1.0000\n",
      "Epoch 93/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1608 - acc: 1.0000\n",
      "Epoch 94/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1517 - acc: 1.0000\n",
      "Epoch 95/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1432 - acc: 1.0000\n",
      "Epoch 96/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.1354 - acc: 1.0000\n",
      "Epoch 97/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.1281 - acc: 1.0000\n",
      "Epoch 98/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1213 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1150 - acc: 1.0000\n",
      "Epoch 100/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.1092 - acc: 1.0000\n",
      "Epoch 101/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.1038 - acc: 1.0000\n",
      "Epoch 102/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0987 - acc: 1.0000\n",
      "Epoch 103/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0941 - acc: 1.0000\n",
      "Epoch 104/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0897 - acc: 1.0000\n",
      "Epoch 105/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0856 - acc: 1.0000\n",
      "Epoch 106/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0818 - acc: 1.0000\n",
      "Epoch 107/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0783 - acc: 1.0000\n",
      "Epoch 108/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0750 - acc: 1.0000\n",
      "Epoch 109/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0719 - acc: 1.0000\n",
      "Epoch 110/240\n",
      "16/16 [==============================] - 0s 62us/step - loss: 0.0691 - acc: 1.0000\n",
      "Epoch 111/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 112/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0638 - acc: 1.0000\n",
      "Epoch 113/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0615 - acc: 1.0000\n",
      "Epoch 114/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0592 - acc: 1.0000\n",
      "Epoch 115/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0572 - acc: 1.0000\n",
      "Epoch 116/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0552 - acc: 1.0000\n",
      "Epoch 117/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0533 - acc: 1.0000\n",
      "Epoch 118/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0516 - acc: 1.0000\n",
      "Epoch 119/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0499 - acc: 1.0000\n",
      "Epoch 120/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0484 - acc: 1.0000\n",
      "Epoch 121/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0469 - acc: 1.0000\n",
      "Epoch 122/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0455 - acc: 1.0000\n",
      "Epoch 123/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0441 - acc: 1.0000\n",
      "Epoch 124/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0429 - acc: 1.0000\n",
      "Epoch 125/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0417 - acc: 1.0000\n",
      "Epoch 126/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0405 - acc: 1.0000\n",
      "Epoch 127/240\n",
      "16/16 [==============================] - 0s 563us/step - loss: 0.0395 - acc: 1.0000\n",
      "Epoch 128/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 129/240\n",
      "16/16 [==============================] - 0s 438us/step - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 130/240\n",
      "16/16 [==============================] - 0s 375us/step - loss: 0.0365 - acc: 1.0000\n",
      "Epoch 131/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0356 - acc: 1.0000\n",
      "Epoch 132/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0347 - acc: 1.0000\n",
      "Epoch 133/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 134/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 135/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0324 - acc: 1.0000\n",
      "Epoch 136/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 137/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0309 - acc: 1.0000\n",
      "Epoch 138/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0303 - acc: 1.0000\n",
      "Epoch 139/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 140/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 141/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0284 - acc: 1.0000\n",
      "Epoch 142/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0278 - acc: 1.0000\n",
      "Epoch 143/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 144/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0267 - acc: 1.0000\n",
      "Epoch 145/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 146/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.0257 - acc: 1.0000\n",
      "Epoch 147/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 148/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 149/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0243 - acc: 1.0000\n",
      "Epoch 150/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0239 - acc: 1.0000\n",
      "Epoch 151/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0234 - acc: 1.0000\n",
      "Epoch 152/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0230 - acc: 1.0000\n",
      "Epoch 153/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0226 - acc: 1.0000\n",
      "Epoch 154/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 155/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 156/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0215 - acc: 1.0000\n",
      "Epoch 157/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 158/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 159/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 160/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 161/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0198 - acc: 1.0000\n",
      "Epoch 162/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 163/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 164/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0189 - acc: 1.0000\n",
      "Epoch 165/240\n",
      "16/16 [==============================] - 0s 313us/step - loss: 0.0186 - acc: 1.0000\n",
      "Epoch 166/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 167/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 168/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 169/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 170/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 171/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 172/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 173/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0166 - acc: 1.0000\n",
      "Epoch 174/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 175/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0161 - acc: 1.0000\n",
      "Epoch 176/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 177/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 178/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 179/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 180/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 181/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 182/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0147 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 184/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 185/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0141 - acc: 1.0000\n",
      "Epoch 186/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 187/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 188/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 189/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 190/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 191/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 192/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 193/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 194/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 195/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 196/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0124 - acc: 1.0000\n",
      "Epoch 197/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 198/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 199/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 200/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 201/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0117 - acc: 1.0000\n",
      "Epoch 202/240\n",
      "16/16 [==============================] - 0s 63us/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 203/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 204/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 205/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 206/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 207/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0109 - acc: 1.0000\n",
      "Epoch 208/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 209/240\n",
      "16/16 [==============================] - 0s 438us/step - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 210/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 211/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 212/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 213/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0103 - acc: 1.0000\n",
      "Epoch 214/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 215/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0101 - acc: 1.0000\n",
      "Epoch 216/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 217/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 218/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 219/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 220/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 221/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 222/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 223/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 224/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0092 - acc: 1.0000\n",
      "Epoch 225/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 226/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 227/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 228/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 229/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 230/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 231/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 232/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 233/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 234/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 235/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0083 - acc: 1.0000\n",
      "Epoch 236/240\n",
      "16/16 [==============================] - 0s 250us/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 237/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 238/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0081 - acc: 1.0000\n",
      "Epoch 239/240\n",
      "16/16 [==============================] - 0s 125us/step - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 240/240\n",
      "16/16 [==============================] - 0s 188us/step - loss: 0.0079 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# the training output will be the category corresponding to the position of the pixel in the mangled image\n",
    "y_train = [np.where(Mmang_tab[ii,:]==1)[0].item(0) for ii in idx]\n",
    "y_train = to_categorical(y_train, nbpix**2)\n",
    "\n",
    "# or the position of the pixel in the corrected image\n",
    "y_train_cor = [np.where(Mstar_tab[ii,:]==1)[0].item(0) for ii in idx]\n",
    "y_train_cor = to_categorical(y_train_cor, nbpix**2)\n",
    "\n",
    "# creating the models\n",
    "model_mangler = Sequential()\n",
    "model_mangler.add(Dense(6*nbpix**2, activation='relu', input_dim=nbpix**2))\n",
    "model_mangler.add(Dense(6*nbpix**2, activation='relu'))\n",
    "model_mangler.add(Dense(nbpix**2, activation='softmax'))\n",
    "# Inversion strategy doesnt work very well...\n",
    "# we are going to try using another model, trained on the corrected input.\n",
    "model_corrector = Sequential()\n",
    "model_corrector.add(Dense(6*nbpix**2, activation='relu', input_dim=nbpix**2))\n",
    "model_corrector.add(Dense(6*nbpix**2, activation='relu'))\n",
    "model_corrector.add(Dense(nbpix**2, activation='softmax'))\n",
    "\n",
    "# Compile the models\n",
    "model_mangler.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=['accuracy'])\n",
    "model_corrector.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# train the model \"mangler\"\n",
    "model_mangler.fit( X_train,\n",
    "    y_train,\n",
    "    epochs=15*nbpix**2,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# train the corrected input model\n",
    "model_corrector.fit( X_train,\n",
    "    y_train_cor,\n",
    "    epochs=15*nbpix**2,\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")    \n",
    "\n",
    "# model_mangler.summary()\n",
    "# model_corrector.summary()\n",
    "\n",
    "# Save the models\n",
    "model_mangler.save(f\"{transform_type}_{training_type}_mangler_{nbpix}x{nbpix}.h5\")\n",
    "model_corrector.save(f\"{transform_type}_{training_type}_corrector_{nbpix}x{nbpix}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the models to an existing image\n",
    "\n",
    "# Load the saved models\n",
    "model_mangler = load_model(f\"{transform_type}_{training_type}_mangler_{nbpix}x{nbpix}.h5\")\n",
    "model_corrector = load_model(f\"{transform_type}_{training_type}_corrector_{nbpix}x{nbpix}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mreal = create_door_img(nbpix) \n",
    "cut = 0.5\n",
    "Mreal = np.random.choice([0, 1], size=(nbpix,nbpix), p=[ 1-cut, cut])\n",
    "# Mreal_split = split_matrix(Mreal)\n",
    "\n",
    "# Predict the mangled image based on the real one\n",
    "Mmang_ml = perform_prediction(Mreal,model_mangler ) # predict_mangler(Mreal)\n",
    "# Predict the corrected input based on the real image\n",
    "Mcor_ml = perform_prediction(Mreal, model_corrector)# predict_corrector(Mreal)\n",
    "# Prediction of the output from the corrected input\n",
    "Mout_ml = perform_prediction(np.round(Mcor_ml), model_mangler)\n",
    "# plot the results\n",
    "# vis_matrices(Mreal, Mmang_ml,Mcor_ml, Mout_ml)\n",
    "if transform_type is \"translation\":\n",
    "    vis_matrices(Mreal, Mmang_ml,Mcor_ml, translate2left(Mcor_ml))\n",
    "elif transform_type is \"rotation\":\n",
    "    vis_matrices(Mreal, Mmang_ml,Mcor_ml, rot90ccw(Mcor_ml))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_matrices(Mreal, rot90ccw(Mreal),rot90cw(Mreal), rot90ccw(rot90cw(Mreal)))\n",
    "\n",
    "from flask import jsonify\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import codecs, json \n",
    "\n",
    "a = np.arange(10).reshape(2,5) # a 2 by 5 array\n",
    "b = a.tolist() # nested lists with same data, indices\n",
    "file_path = \"/path.json\" ## your path variable\n",
    "json.dump(b, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4) ### this saves the array in .json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "{\"mat\": [[0, 0, 0, 1], [0, 0, 1, 1], [1, 1, 0, 0], [0, 0, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(a.shape)\n",
    "\n",
    "# json_dump = json.dumps({'a': a, 'aa': [2, (2, 3, 4), a], 'bb': [2]}, cls=NumpyEncoder)\n",
    "json_dump = json.dumps({\"mat\": Mreal}, cls=NumpyEncoder)\n",
    "print(json_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [1 1 1 0]\n",
      " [1 1 1 0]]\n",
      "{\"a\": [[1, 0, 0, 1], [0, 1, 0, 1], [1, 0, 1, 1], [1, 1, 0, 1]]}\n"
     ]
    }
   ],
   "source": [
    "cut = 0.5\n",
    "print(nbpix)\n",
    "rmd_img = np.random.choice([0, 1], size=(nbpix,nbpix), p=[ 1-cut, cut])\n",
    "print(np.random.choice([0, 1], size=(nbpix,nbpix), p=[ 1-cut, cut]))\n",
    "json_dump = json.dumps({\"a\": rmd_img} , cls=NumpyEncoder)\n",
    "print(json_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"{\\\\\"a\\\\\": [[1, 0, 0, 1], [0, 1, 0, 1], [1, 0, 1, 1], [1, 1, 0, 1]]}\"]'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(json_dump).to_json(orient='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrayToList(arr):\n",
    "    return [ii.item() for ii in arr.reshape((arr.size,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# transform into a list\n",
    "mat_list = [ii.item() for ii in Mreal.reshape((nbpix**2,1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1]\n",
      " [0 0 1 1]\n",
      " [1 1 0 0]\n",
      " [0 0 1 1]]\n",
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayToList(Mreal)\n",
    "# Mreal.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nto interface with the current application object in some way. To solve\nthis, set up an application context with app.app_context().  See the\ndocumentation for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-afe2d66e4dfe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjsonify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\flask\\json\\__init__.py\u001b[0m in \u001b[0;36mjsonify\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mseparators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m':'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcurrent_app\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'JSONIFY_PRETTYPRINT_REGULAR'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcurrent_app\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         \u001b[0mindent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0mseparators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m', '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\werkzeug\\local.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__members__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_current_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\werkzeug\\local.py\u001b[0m in \u001b[0;36m_get_current_object\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \"\"\"\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__release_local__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__local\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\flask\\globals.py\u001b[0m in \u001b[0;36m_find_app\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mtop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_app_ctx_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_app_ctx_err_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nto interface with the current application object in some way. To solve\nthis, set up an application context with app.app_context().  See the\ndocumentation for more information."
     ]
    }
   ],
   "source": [
    "jsonify(mat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
